{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfd5b69-73a1-40b6-9a98-54220aef7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow import keras\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688b21cf-f917-45cd-9b3e-3f0936062f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../data/liuliang_data\")\n",
    "from features_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14ebc28a-d73a-4fd1-9b64-116dba14b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Process_Layer(layers.Layer):\n",
    "    def __init__(self, sparse_features, dense_features, price_features):\n",
    "        super().__init__()\n",
    "        self.sparse_features = sparse_features\n",
    "        self.dense_features = dense_features\n",
    "        self.price_features = price_features\n",
    "        self.concat_layer = layers.Concatenate()  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        concat_numeric = []\n",
    "        for name, input in inputs.items():\n",
    "            if name in self.dense_features:\n",
    "                input_cast = tf.cast(input, tf.float32)  \n",
    "                if name not in self.price_features:\n",
    "                    temp_feature = tf.floor( tf.math.log1p(input_cast + 1) / tf.math.log(tf.constant(2.0, dtype=tf.float32)) )\n",
    "                else:\n",
    "                    temp_feature = tf.floor( tf.math.log1p(input_cast + 9) / tf.math.log(tf.constant(10.0, dtype=tf.float32)) )\n",
    "                temp_feature = tf.expand_dims(temp_feature, 1)\n",
    "                concat_numeric.append(temp_feature)\n",
    "\n",
    "        return self.concat_layer(concat_numeric)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5935f74b-489a-4ca8-9c07-8ba89cb2d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(layers.Layer):\n",
    "    def __init__(self, units = [256, 64]):\n",
    "        super().__init__()\n",
    "        self.dnn = keras.Sequential([\n",
    "            layers.Dense(unit, activation = 'relu') for unit in units\n",
    "        ])\n",
    "    def call(self, x):\n",
    "        return self.dnn(x)\n",
    "        \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MultiLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiLoss, self).__init__(**kwargs)\n",
    "        self.bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "    def call(self, y_true, y_pred_mask):\n",
    "        y_pred, mask = y_pred_mask  # 解包 y_pred 和 mask\n",
    "        return self.compute_loss(y_true, y_pred, mask)\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred, mask):  \n",
    "        sum_loss = 0.0\n",
    "        batch_size = tf.shape(y_true)[0]\n",
    "        num_classes = tf.shape(y_true)[1]\n",
    "        count = tf.constant(0.0, dtype=tf.float32)  # 初始化 count 为 float32 Tensor\n",
    "\n",
    "        for j in range(num_classes):\n",
    "            tp_yhat = tf.expand_dims(y_pred[:, j], axis=1)\n",
    "            tp_y = tf.expand_dims(y_true[:, j], axis=1)\n",
    "            sample_weight = tf.expand_dims(mask[:, j], axis=1)\n",
    "\n",
    "            sum_loss += self.bce_loss(tp_y, tp_yhat, sample_weight=sample_weight)\n",
    "            count += 1 # 确保类型一致\n",
    "    \n",
    "        return sum_loss / count if count > 0 else tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "        \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class EveryDayModel(Model):\n",
    "    def __init__(self, sparse_features, dense_features, price_features, label_cols, units = [256, 128, 64]):\n",
    "        super().__init__()\n",
    "        self.embedding_dict = {}\n",
    "        self.sparse_features = sparse_features\n",
    "        self.label_cols = label_cols\n",
    "        num_bins = 10000\n",
    "        for name in sparse_features:\n",
    "            self.embedding_dict[name] = layers.Embedding(num_bins, 8, name = name)\n",
    "        self.dense_process_layer = Dense_Process_Layer(sparse_features, dense_features, price_features)\n",
    "        self.concat_embedding = layers.Concatenate()\n",
    "\n",
    "        # 多任务 \n",
    "        self.dnn = DNN(units )\n",
    "\n",
    "        self.day1 = keras.layers.Dense(1)\n",
    "        self.day2 = keras.layers.Dense(1)\n",
    "        self.day3 = keras.layers.Dense(1)\n",
    "        self.day4 = keras.layers.Dense(1)\n",
    "        self.day5 = keras.layers.Dense(1)\n",
    "        self.day6 = keras.layers.Dense(1)\n",
    "        self.day7 = keras.layers.Dense(1)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        dense_input = self.dense_process_layer(inputs)\n",
    "        embeddings = [dense_input]\n",
    "\n",
    "        for name, input in inputs.items():\n",
    "            if name in self.embedding_dict:\n",
    "                temp_embd = self.embedding_dict[name](input)\n",
    "                embeddings.append(temp_embd)\n",
    "        embedding_input = self.concat_embedding(embeddings)\n",
    "\n",
    "        base_out_put = self.dnn(embedding_input)\n",
    "        # print(\" mid_output: \",base_out_put)\n",
    "        logit_7 = tf.sigmoid(self.day7(base_out_put))\n",
    "        \n",
    "        logit_1 = tf.sigmoid(self.day1(base_out_put) * logit_7)\n",
    "        logit_2 = tf.sigmoid(self.day2(base_out_put) * logit_7)\n",
    "        logit_3 = tf.sigmoid(self.day3(base_out_put) * logit_7)\n",
    "        logit_4 = tf.sigmoid(self.day4(base_out_put) * logit_7)\n",
    "        logit_5 = tf.sigmoid(self.day5(base_out_put) * logit_7)\n",
    "        logit_6 = tf.sigmoid(self.day6(base_out_put) * logit_7)\n",
    "\n",
    "        \n",
    "        return layers.Concatenate()([logit_1, logit_2, logit_3, logit_4, logit_5, logit_6, logit_7])\n",
    "    def train_step(self, inputs):\n",
    "        labels = []\n",
    "        mask_s = inputs['mask']\n",
    "  \n",
    "        for lc_name in label_col:\n",
    "            labeli = tf.expand_dims(inputs[lc_name], 1)\n",
    "            labels.append(labeli)\n",
    "        labels = layers.concatenate(labels)\n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self(inputs)\n",
    "            loss = tf.reduce_mean(self.loss(labels, (preds, mask_s)))\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        results = {}\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "        \n",
    "    def evaluate(self, x, y=None, batch_size=None, steps=None, **kwargs):\n",
    "        dataset = x\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "    \n",
    "        # 遍历数据集中的每个批次\n",
    "        for inputs in dataset:\n",
    "            labels = []\n",
    "            mask_s = inputs['mask']\n",
    "    \n",
    "            # 拼接标签\n",
    "            for lc_name in label_col:\n",
    "                labeli = tf.expand_dims(inputs[lc_name], 1)\n",
    "                labels.append(labeli)\n",
    "            labels = layers.concatenate(labels)\n",
    "    \n",
    "            # 计算模型预测\n",
    "            preds = self(inputs)\n",
    "            loss = tf.reduce_mean(self.loss(labels, (preds, mask_s)))\n",
    "    \n",
    "            # 累加损失\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "    \n",
    "        # 计算平均损失\n",
    "        avg_loss = total_loss / num_batches\n",
    "        results = {'loss': avg_loss}\n",
    "        return results    \n",
    "    def predict(self, inputs):\n",
    "        dataset = inputs\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "    \n",
    "        for inputs in dataset:\n",
    "            # 获取标签\n",
    "            labels = inputs['label']  \n",
    "    \n",
    "            # 计算模型预测\n",
    "            preds = self(inputs)[:, -1]\n",
    "    \n",
    "            # 计算下单期望\n",
    "            pred_orders = tf.reduce_sum(preds)\n",
    "            true_orders = tf.reduce_sum(labels)\n",
    "    \n",
    "            # **修正：确保数据类型一致**\n",
    "            true_orders = tf.cast(true_orders, dtype=tf.float32)\n",
    "            bias = (pred_orders - true_orders) / true_orders\n",
    "    \n",
    "            # 计算 AUC\n",
    "            auc = roc_auc_score(y_true=labels.numpy(), y_score=preds.numpy())  # 转换为 numpy 以适配 sklearn\n",
    "    \n",
    "            result = {\n",
    "                'bias': bias.numpy(),  # 转换为 numpy 以避免 TensorFlow 计算图问题\n",
    "                'AUC': auc,\n",
    "                'preds': preds[:10],\n",
    "                'true_orders': true_orders,\n",
    "                'pred_orders':pred_orders\n",
    "            }\n",
    "            \n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20125f68-f7c4-4278-8d25-d3e8ace49f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_delta_date_str(date_str, delta):\n",
    "    return  (datetime.datetime.strptime(date_str, \"%Y-%m-%d\") + datetime.timedelta(days=delta)).strftime('%Y-%m-%d')\n",
    "\n",
    "def str2date(date_str):\n",
    "    return datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "def date2str(date):\n",
    "    return date.strftime('%Y-%m-%d')\n",
    "\n",
    "def get_train_test_data(data, startdate, enddate, testdate):\n",
    "    train_data = data[\n",
    "        (data['activate_date'] >= startdate) & (data['activate_date'] <= enddate)].reset_index(\n",
    "        drop=True).copy() \n",
    "\n",
    "    test_data = data[\n",
    "        (data['activate_date'] == testdate)].reset_index(drop=True).copy()  \n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def process_mask_col(train_data, mask_dates):\n",
    "    shape = len(train_data)\n",
    "    train_data['mask'] = [np.ones((7,))] * shape\n",
    "\n",
    "    train_data, recent_data = train_data[~train_data.dt.isin(mask_dates)] , train_data[train_data.dt.isin(mask_dates)]\n",
    "    train_data, valid_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "    \n",
    "    \n",
    "    for i, msk_dt in enumerate(mask_dates):\n",
    "        temp_mask = np.concatenate([np.ones((i + 1,)) , np.zeros((7 - i - 1,))])\n",
    "        recent_data.loc[recent_data.dt == msk_dt, 'mask'] = recent_data.loc[recent_data.dt == msk_dt].apply(lambda row: temp_mask, axis = 1)\n",
    "    train_data = pd.concat([train_data, recent_data], ignore_index=True)\n",
    "    print(\"数据集信息...\" )\n",
    "    # print(f\"训练集： {train_data.dt.value_counts()}, 验证集： {valid_data.dt.value_counts()}\")\n",
    "    return train_data , valid_data\n",
    "\n",
    "def create_tf_dataset(data, features, batch):\n",
    "    train_inputs = {name: tf.constant(v.values) if name != 'mask' else tf.constant(np.array(v.tolist())) \n",
    "                    for name, v in data[features + label_col].items()}\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_inputs)                                    \n",
    "    return train_dataset.batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f002ae4-e126-44c5-9a8f-08af5274ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练基本配置 \n",
    "res_csv = None\n",
    "folder_name = 'deep_res'\n",
    "Pred_Days = 10\n",
    "Pred_Date = '2023-05-01'\n",
    "label_col = ['label_1','label_2','label_3','label_4','label_5','label_6','label']\n",
    "# 处理缺失值\n",
    "import platform \n",
    "if platform.system() == 'Darwin':\n",
    "    data = pd.read_csv(\"../data/liuliang_data/完整toy_liuliang_data.csv\", index_col= 0)\n",
    "else:\n",
    "    data = pd.read_csv(\"../data/liuliang_data/toy_liuliang_data.csv\", index_col= 0)\n",
    "data.loc[:, features] = data.loc[:, features].fillna(0)\n",
    "\n",
    "train_days = 20\n",
    "\n",
    "dense_features = [feature for feature in features if feature not in category_features]\n",
    "sparse_features, dense_features, price_feature = category_features, dense_features, price_fatures\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 设置 EarlyStopping 回调\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa257e-b661-4ec9-9ce8-d60bd766f110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集开始：2023-04-04, 训练集结束：2023-04-30, 测试集：2023-05-01\n",
      "数据集信息...\n",
      "Epoch 1/25\n",
      "104/104 [==============================] - 6s 39ms/step - loss: 0.2876 - val_loss: 0.2329\n",
      "Epoch 2/25\n",
      "104/104 [==============================] - 4s 40ms/step - loss: 0.1467 - val_loss: 0.1735\n",
      "Epoch 3/25\n",
      " 82/104 [======================>.......] - ETA: 0s - loss: 0.1470"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_model(data):\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',       \n",
    "        patience=3,               \n",
    "        restore_best_weights=True )\n",
    "\n",
    "    for delta in range(0, Pred_Days):\n",
    "        delta_sample_days = train_days\n",
    "        \n",
    "        today = get_delta_date_str(Pred_Date, delta)\n",
    "        startdate, enddate, testdate = get_delta_date_str(today, -train_days - 7), get_delta_date_str(today, -1), today\n",
    "        \n",
    "        \n",
    "        print(f\"训练集开始：{startdate}, 训练集结束：{enddate}, 测试集：{testdate}\")\n",
    "        \n",
    "        #  在 mask_date中间的 全部用mask 日期填充\n",
    "        mask_dates = [date2str(j) for j in pd.date_range(str2date(get_delta_date_str(today, -1)),periods=6,freq='-1D')]\n",
    "\n",
    "        # 按照时间区间 获取 训练 测试集 \n",
    "        train_data, test_data = get_train_test_data(data, startdate, enddate, testdate)\n",
    "        # 给 train data 添加mask 列\n",
    "        train_data, valid_data = process_mask_col(train_data, mask_dates)\n",
    "\n",
    "        train_dataset = create_tf_dataset(train_data, features + ['mask'], 2048)\n",
    "        valid_dataset = create_tf_dataset(valid_data, features + ['mask'], 2048)\n",
    "        test_data = create_tf_dataset(test_data, features, 200000)\n",
    "        \n",
    "        # MODEL \n",
    "        model = EveryDayModel(sparse_features, dense_features, price_feature, label_col,[128,64] )\n",
    "        model.compile(loss = MultiLoss(), optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003) )\n",
    "\n",
    "        # debug \n",
    "        # model.fit(valid_dataset)\n",
    "        # print(model.evaluate(valid_dataset))\n",
    "        \n",
    "        model.fit(train_dataset, validation_data = valid_dataset, epochs=25,callbacks=[early_stopping]) \n",
    "        \n",
    "        # model.fit(valid_dataset, validation_data = valid_dataset, epochs=1, callbacks=[early_stopping])\n",
    "        print(model.predict(test_data))\n",
    "        return train_dataset\n",
    "\n",
    "\n",
    "t1 = train_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4150e3d-f2c9-41fd-a19c-2e568e9c885d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ccfb9-7887-4fd2-802b-b017d5554e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,1,1,1,1]  [0.2,0.1,0.3,0.5,0.8]  [1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f2012-a089-4768-b6fe-9da7a4ebf80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
