{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfd5b69-73a1-40b6-9a98-54220aef7a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\conda\\envs\\tfgpu215\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow import keras\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688b21cf-f917-45cd-9b3e-3f0936062f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../data/liuliang_data\")\n",
    "from features_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ebc28a-d73a-4fd1-9b64-116dba14b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Process_Layer(layers.Layer):\n",
    "    def __init__(self, sparse_features, dense_features, price_features):\n",
    "        super().__init__()\n",
    "        self.sparse_features = sparse_features\n",
    "        self.dense_features = dense_features\n",
    "        self.price_features = price_features\n",
    "        self.concat_layer = layers.Concatenate()  # Specifying axis in constructor\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        concat_numeric = []\n",
    "        for name, input in inputs.items():\n",
    "            if name in self.dense_features:\n",
    "                input_cast = tf.cast(input, tf.float32)  # Cast input once\n",
    "                if name not in self.price_features:\n",
    "                    temp_feature = tf.math.log1p(input_cast) / tf.math.log(tf.constant(2.0, dtype=tf.float32))\n",
    "                else:\n",
    "                    temp_feature = tf.math.log1p(input_cast) / tf.math.log(tf.constant(10.0, dtype=tf.float32))\n",
    "                temp_feature = tf.expand_dims(temp_feature, 1)\n",
    "                concat_numeric.append(temp_feature)\n",
    "\n",
    "        return self.concat_layer(concat_numeric)  # No need to specify axis again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5935f74b-489a-4ca8-9c07-8ba89cb2d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(layers.Layer):\n",
    "    def __init__(self, units = [256, 64]):\n",
    "        super().__init__()\n",
    "        self.dnn = keras.Sequential([\n",
    "            layers.Dense(unit, activation = 'relu') for unit in units\n",
    "        ])\n",
    "    def call(self, x):\n",
    "        return self.dnn(x)\n",
    "        \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class MultiLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiLoss, self).__init__(**kwargs)\n",
    "        self.bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "    def call(self, y_true, y_pred, mask):\n",
    "        sum_loss = 0.0\n",
    "        batch_size = tf.shape(y_true)[0]\n",
    "        num_classes = tf.shape(y_true)[1]\n",
    "\n",
    "        # mask 的形状是 (batch_size, num_classes)，按样本粒度进行处理\n",
    "        for i in range(batch_size):\n",
    "            for j in range(num_classes):\n",
    "                # 对每个样本的每个类别进行检查\n",
    "                if mask[i, j] == 1:\n",
    "                    tp_yhat = tf.expand_dims(y_pred[i, j], axis=0)  # 取出当前样本和类别的预测值\n",
    "                    tp_y = tf.expand_dims(y_true[i, j], axis=0)     # 取出当前样本和类别的真实值\n",
    "                    sum_loss += self.bce_loss(tp_y, tp_yhat)\n",
    "\n",
    "        return sum_loss\n",
    "\n",
    "\n",
    "\n",
    "class EveryDayModel(Model):\n",
    "    def __init__(self, sparse_features, dense_features, price_features, label_cols):\n",
    "        super().__init__()\n",
    "        self.embedding_dict = {}\n",
    "        self.sparse_features = sparse_features\n",
    "        self.label_cols = label_cols\n",
    "        num_bins = 1000\n",
    "        for name in sparse_features:\n",
    "            self.embedding_dict[name] = layers.Embedding(num_bins, 8, name = name)\n",
    "        self.dense_process_layer = Dense_Process_Layer(sparse_features, dense_features, price_features)\n",
    "        self.concat_embedding = layers.Concatenate()\n",
    "\n",
    "        # 多任务 \n",
    "        self.dnn = DNN([256, 64] )\n",
    "\n",
    "        self.day1 = keras.layers.Dense(1)\n",
    "        self.day2 = keras.layers.Dense(1)\n",
    "        self.day3 = keras.layers.Dense(1)\n",
    "        self.day4 = keras.layers.Dense(1)\n",
    "        self.day5 = keras.layers.Dense(1)\n",
    "        self.day6 = keras.layers.Dense(1)\n",
    "        self.day7 = keras.layers.Dense(1)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        dense_input = self.dense_process_layer(inputs)\n",
    "        embeddings = [dense_input]\n",
    "\n",
    "        for name, input in inputs.items():\n",
    "            if name in self.embedding_dict:\n",
    "                temp_embd = self.embedding_dict[name](input)\n",
    "                embeddings.append(temp_embd)\n",
    "        embedding_input = self.concat_embedding(embeddings)\n",
    "\n",
    "        logit_7 = tf.sigmoid(self.day7(embedding_input))\n",
    "        \n",
    "        logit_1 = tf.sigmoid(self.day1(embedding_input) * logit_7)\n",
    "        logit_2 = tf.sigmoid(self.day2(embedding_input) * logit_7)\n",
    "        logit_3 = tf.sigmoid(self.day3(embedding_input) * logit_7)\n",
    "        logit_4 = tf.sigmoid(self.day4(embedding_input) * logit_7)\n",
    "        logit_5 = tf.sigmoid(self.day5(embedding_input) * logit_7)\n",
    "        logit_6 = tf.sigmoid(self.day6(embedding_input) * logit_7)\n",
    "\n",
    "        \n",
    "        return layers.Concatenate()([logit_1, logit_2, logit_3, logit_4, logit_5, logit_6, logit_7])\n",
    "    def train_step(self, inputs):\n",
    "        labels = []\n",
    "        mask_s = inputs['mask']\n",
    "  \n",
    "        for lc_name in label_col:\n",
    "            labeli = tf.expand_dims(inputs[lc_name], 1)\n",
    "            labels.append(labeli)\n",
    "        labels = layers.concatenate(labels)\n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self(inputs)\n",
    "            loss = tf.reduce_mean(self.loss(preds, labels, masks))\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        self.compiled_metrics.update_state(label, predict)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20125f68-f7c4-4278-8d25-d3e8ace49f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_delta_date_str(date_str, delta):\n",
    "    return  (datetime.datetime.strptime(date_str, \"%Y-%m-%d\") + datetime.timedelta(days=delta)).strftime('%Y-%m-%d')\n",
    "\n",
    "def str2date(date_str):\n",
    "    return datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "def date2str(date):\n",
    "    return date.strftime('%Y-%m-%d')\n",
    "\n",
    "def get_train_test_data(data, startdate, enddate, testdate):\n",
    "    train_data = data[\n",
    "        (data['activate_date'] >= startdate) & (data['activate_date'] <= enddate)].reset_index(\n",
    "        drop=True).copy() \n",
    "\n",
    "    test_data = data[\n",
    "        (data['activate_date'] == testdate)].reset_index(drop=True).copy()  \n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def process_mask_col(train_data, mask_dates):\n",
    "    shape = len(train_data)\n",
    "    train_data['mask'] = [np.ones((7,))] * shape\n",
    "    for i, msk_dt in enumerate(mask_dates):\n",
    "        temp_mask = np.concatenate([np.ones((i + 1,)) , np.zeros((7 - i - 1,))])\n",
    "        train_data.loc[train_data.dt == msk_dt, 'mask'] = train_data.loc[train_data.dt == msk_dt].apply(lambda row: temp_mask, axis = 1)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f002ae4-e126-44c5-9a8f-08af5274ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练基本配置 \n",
    "res_csv = None\n",
    "folder_name = 'deep_res'\n",
    "Pred_Days = 10\n",
    "Pred_Date = '2023-05-01'\n",
    "label_col = ['label_1','label_2','label_3','label_4','label_5','label_6','label']\n",
    "# 处理缺失值\n",
    "import platform \n",
    "if platform.os == 'Windows':\n",
    "    data = pd.read_csv(\"../data/liuliang_data/完整toy_liuliang_data.csv\", index_col= 0)\n",
    "else:\n",
    "    data = pd.read_csv(\"../data/liuliang_data/toy_liuliang_data.csv\", index_col= 0)\n",
    "data.loc[:, features] = data.loc[:, features].fillna(0)\n",
    "\n",
    "train_days = 13\n",
    "dense_features = [feature for feature in features if feature not in category_features]\n",
    "sparse_features, dense_features, price_feature = category_features, dense_features, price_fatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bfa257e-b661-4ec9-9ce8-d60bd766f110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集开始：2023-04-11, 训练集结束：2023-04-30, 测试集：2023-05-01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_model(data):\n",
    " \n",
    "    for delta in range(0, Pred_Days):\n",
    "        delta_sample_days = train_days\n",
    "        \n",
    "        today = get_delta_date_str(Pred_Date, delta)\n",
    "        startdate, enddate, testdate = get_delta_date_str(today, -train_days - 7), get_delta_date_str(today, -1), today\n",
    "        \n",
    "        \n",
    "        print(f\"训练集开始：{startdate}, 训练集结束：{enddate}, 测试集：{testdate}\")\n",
    "        \n",
    "        #  在 mask_date中间的 全部用mask 日期填充\n",
    "        mask_dates = [date2str(j) for j in pd.date_range(str2date(get_delta_date_str(today, -1)),periods=6,freq='-1D')]\n",
    "\n",
    "        # 按照时间区间 获取 训练 测试集 \n",
    "        train_data, test_data = get_train_test_data(data, startdate, enddate, testdate)\n",
    "        # 给 train data 添加mask 列\n",
    "        train_data = process_mask_col(train_data, mask_dates)\n",
    "        train_inputs = {name: tf.constant(v.values) if name != 'mask' else tf.constant(np.array(v.tolist())) \n",
    "                        for name, v in train_data[features + label_col].items()}\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_inputs)\n",
    "        train_dataset = train_dataset.batch(512)\n",
    "\n",
    "        # MODEL \n",
    "        model = EveryDayModel(sparse_features, dense_features, price_feature, label_col )\n",
    "        return train_dataset\n",
    "        \n",
    "\n",
    "\n",
    "t1 = train_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70ac4e99-715f-4460-a6c7-1bbffbdbafdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], shape=(512, 7), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inputs in t1:\n",
    "    label_cols = []\n",
    "    for lc_name in label_col:\n",
    "        labeli = tf.expand_dims(inputs[lc_name], 1)\n",
    "        label_cols.append(labeli)\n",
    "    res = layers.concatenate(label_cols)\n",
    "    print(res)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca8124-8fcc-4267-9ddd-ef6a83f5d074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd675336-17c2-4bff-971b-bad23f821489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6c11b3-3082-4276-8216-b8a1359ceea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c844fa2-f9e4-481b-9d2f-262ccd8316d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {name:tf.constant(v.values) for name,v in data.items() if name in features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f58e240-e3b7-4152-9652-ae7d04fdcde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20000, 7), dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa9133-ac10-40e8-82fc-ac95acf17836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "517e92eb-e1ca-46c9-abb2-7ea8c9b1beee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 1 1 0 1 1]\n",
      " [1 1 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 1]\n",
      " [1 1 1 1 0 0 1]\n",
      " [0 0 1 0 1 1 0]\n",
      " [0 1 0 1 1 0 1]\n",
      " [0 0 0 1 0 1 1]\n",
      " [0 1 0 1 1 1 0]\n",
      " [1 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 0 0]\n",
      " [0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 1 1]\n",
      " [0 1 1 0 0 1 1]\n",
      " [1 0 1 1 0 1 0]\n",
      " [0 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 0 1]\n",
      " [1 1 0 1 1 0 1]\n",
      " [0 1 0 1 0 1 1]\n",
      " [1 0 0 0 1 0 1]\n",
      " [1 1 1 1 0 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [1 1 0 0 0 1 0]\n",
      " [1 1 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0]\n",
      " [1 1 1 0 1 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 0 1 1 0 0 0]\n",
      " [0 0 1 1 0 0 0]\n",
      " [0 0 1 1 0 1 0]\n",
      " [1 1 0 0 0 0 0]\n",
      " [1 1 1 0 0 1 1]\n",
      " [0 1 0 1 1 0 1]], shape=(32, 7), dtype=int32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ec18a92-2803-4627-a483-afdbec2e48bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0a0f25d-b14c-464d-8ab7-df3cb9aecd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2c90fda-4c78-47f1-9d8a-3f2de47ad428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([0.7945, 0.9919, 0.8439, 1.1343, 0.6553, 0.7432, 1.2322, 0.9226,\n",
       "       1.4304, 0.8548, 0.874 , 0.9434, 1.0212, 1.0771, 1.3806, 1.0818,\n",
       "       0.9163, 0.8238, 0.7817, 0.8627, 0.7375, 0.853 , 0.8797, 0.8234,\n",
       "       0.8406, 1.0483, 2.0231, 0.6447, 0.7356, 0.8785, 1.0572, 0.7386],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f2012-a089-4768-b6fe-9da7a4ebf80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
