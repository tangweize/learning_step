{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfd5b69-73a1-40b6-9a98-54220aef7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow import keras\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999ed808-2546-4c05-a1f3-6908bd750f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用 GPU 设备： [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "是否使用 GPU： True\n",
      "是否使用 cuDNN： True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"可用 GPU 设备：\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"是否使用 GPU：\", tf.test.is_built_with_cuda())  # 确保 TensorFlow 支持 CUDA\n",
    "print(\"是否使用 cuDNN：\", tf.test.is_built_with_gpu_support())  # 确保 cuDNN 可用\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688b21cf-f917-45cd-9b3e-3f0936062f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../data/liuliang_data\")\n",
    "from features_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14ebc28a-d73a-4fd1-9b64-116dba14b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Process_Layer(layers.Layer):\n",
    "    def __init__(self, sparse_features, dense_features, price_features):\n",
    "        super().__init__()\n",
    "        self.sparse_features = sparse_features\n",
    "        self.dense_features = dense_features\n",
    "        self.price_features = price_features\n",
    "        self.concat_layer = layers.Concatenate()  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        concat_numeric = []\n",
    "        for name, input in inputs.items():\n",
    "            if name in self.dense_features:\n",
    "                input_cast = tf.cast(input, tf.float32)  \n",
    "                if name not in self.price_features:\n",
    "                    temp_feature = tf.floor( tf.math.log1p(input_cast + 1) / tf.math.log(tf.constant(2.0, dtype=tf.float32)) )\n",
    "                else:\n",
    "                    temp_feature = tf.floor( tf.math.log1p(input_cast + 9) / tf.math.log(tf.constant(10.0, dtype=tf.float32)) )\n",
    "                temp_feature = tf.expand_dims(temp_feature, 1)\n",
    "                concat_numeric.append(temp_feature)\n",
    "\n",
    "        return self.concat_layer(concat_numeric)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5935f74b-489a-4ca8-9c07-8ba89cb2d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(layers.Layer):\n",
    "    def __init__(self, units = [256, 64]):\n",
    "        super().__init__()\n",
    "        self.dnn = keras.Sequential([\n",
    "            layers.Dense(unit, activation = 'relu') for unit in units\n",
    "        ])\n",
    "    def call(self, x):\n",
    "        return self.dnn(x)\n",
    "        \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MultiLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiLoss, self).__init__(**kwargs)\n",
    "        self.bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "    def call(self, y_true, y_pred_mask):\n",
    "        y_pred, mask = y_pred_mask  # 解包 y_pred 和 mask\n",
    "        return self.compute_loss(y_true, y_pred, mask)\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred, mask):  \n",
    "        sum_loss = 0.0\n",
    "        batch_size = tf.shape(y_true)[0]\n",
    "        num_classes = tf.shape(y_true)[1]\n",
    "        count = tf.constant(0.0, dtype=tf.float32)  # 初始化 count 为 float32 Tensor\n",
    "\n",
    "        for j in range(num_classes):\n",
    "            tp_yhat = tf.expand_dims(y_pred[:, j], axis=1)\n",
    "            tp_y = tf.expand_dims(y_true[:, j], axis=1)\n",
    "            sample_weight = tf.expand_dims(mask[:, j], axis=1)\n",
    "\n",
    "            sum_loss += self.bce_loss(tp_y, tp_yhat, sample_weight=sample_weight)\n",
    "            count += 1 # 确保类型一致\n",
    "    \n",
    "        return sum_loss / count if count > 0 else tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "        \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class EveryDayModel(Model):\n",
    "    def __init__(self, sparse_features, dense_features, price_features, label_cols, units = [256, 128, 64]):\n",
    "        super().__init__()\n",
    "        self.embedding_dict = {}\n",
    "        self.sparse_features = sparse_features\n",
    "        self.label_cols = label_cols\n",
    "        num_bins = 10000\n",
    "        for name in sparse_features:\n",
    "            self.embedding_dict[name] = layers.Embedding(num_bins, 8, name = name)\n",
    "        self.dense_process_layer = Dense_Process_Layer(sparse_features, dense_features, price_features)\n",
    "        self.concat_embedding = layers.Concatenate()\n",
    "\n",
    "        # 多任务 \n",
    "        self.dnn = DNN(units )\n",
    "\n",
    "        self.day1 = keras.layers.Dense(1)\n",
    "        self.day2 = keras.layers.Dense(1)\n",
    "        self.day3 = keras.layers.Dense(1)\n",
    "        self.day4 = keras.layers.Dense(1)\n",
    "        self.day5 = keras.layers.Dense(1)\n",
    "        self.day6 = keras.layers.Dense(1)\n",
    "        self.day7 = keras.layers.Dense(1)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        dense_input = self.dense_process_layer(inputs)\n",
    "        embeddings = [dense_input]\n",
    "\n",
    "        for name, input in inputs.items():\n",
    "            if name in self.embedding_dict:\n",
    "                temp_embd = self.embedding_dict[name](input)\n",
    "                embeddings.append(temp_embd)\n",
    "        embedding_input = self.concat_embedding(embeddings)\n",
    "\n",
    "        base_out_put = self.dnn(embedding_input)\n",
    "        # print(\" mid_output: \",base_out_put)\n",
    "        logit_7 = tf.sigmoid(self.day7(base_out_put))\n",
    "\n",
    "        \n",
    "        # 弄2025-03-24 两个 prob 试试 \n",
    "        logit_1 = tf.sigmoid(self.day1(base_out_put)) * logit_7\n",
    "        logit_2 = tf.sigmoid(self.day2(base_out_put)) * logit_7\n",
    "        logit_3 = tf.sigmoid(self.day3(base_out_put)) * logit_7\n",
    "        logit_4 = tf.sigmoid(self.day4(base_out_put)) * logit_7\n",
    "        logit_5 = tf.sigmoid(self.day5(base_out_put)) * logit_7\n",
    "        logit_6 = tf.sigmoid(self.day6(base_out_put)) * logit_7\n",
    "\n",
    "        \n",
    "        return layers.Concatenate()([logit_1, logit_2, logit_3, logit_4, logit_5, logit_6, logit_7])\n",
    "    def train_step(self, inputs):\n",
    "        labels = []\n",
    "        mask_s = inputs['mask']\n",
    "  \n",
    "        for lc_name in label_col:\n",
    "            labeli = tf.expand_dims(inputs[lc_name], 1)\n",
    "            labels.append(labeli)\n",
    "        labels = layers.concatenate(labels)\n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self(inputs)\n",
    "            loss = tf.reduce_mean(self.loss(labels, (preds, mask_s)))\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        results = {}\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "        \n",
    "    def evaluate(self, x, y=None, batch_size=None, steps=None, **kwargs):\n",
    "        dataset = x\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "    \n",
    "        # 遍历数据集中的每个批次\n",
    "        for inputs in dataset:\n",
    "            labels = []\n",
    "            mask_s = inputs['mask']\n",
    "    \n",
    "            # 拼接标签\n",
    "            for lc_name in label_col:\n",
    "                labeli = tf.expand_dims(inputs[lc_name], 1)\n",
    "                labels.append(labeli)\n",
    "            labels = layers.concatenate(labels)\n",
    "    \n",
    "            # 计算模型预测\n",
    "            preds = self(inputs)\n",
    "            loss = tf.reduce_mean(self.loss(labels, (preds, mask_s)))\n",
    "    \n",
    "            # 累加损失\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "    \n",
    "        # 计算平均损失\n",
    "        avg_loss = total_loss / num_batches\n",
    "        results = {'loss': avg_loss}\n",
    "        return results    \n",
    "    def predict(self, inputs, pred_index = -1):\n",
    "        dataset = inputs\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "    \n",
    "        for inputs in dataset:\n",
    "            # 获取标签\n",
    "            labels = inputs[label_col[pred_index]]  \n",
    "    \n",
    "            # 计算模型预测\n",
    "            preds = self(inputs)[:, pred_index]\n",
    "    \n",
    "            # 计算下单期望\n",
    "            pred_orders = tf.reduce_sum(preds)\n",
    "            true_orders = tf.reduce_sum(labels)\n",
    "    \n",
    "            # **修正：确保数据类型一致**\n",
    "            true_orders = tf.cast(true_orders, dtype=tf.float32)\n",
    "            bias = (pred_orders - true_orders) / true_orders\n",
    "    \n",
    "            # 计算 AUC\n",
    "            auc = roc_auc_score(y_true=labels.numpy(), y_score=preds.numpy())  # 转换为 numpy 以适配 sklearn\n",
    "    \n",
    "            result = {\n",
    "                'bias': bias.numpy(),  # 转换为 numpy 以避免 TensorFlow 计算图问题\n",
    "                'AUC': auc,\n",
    "                'preds': preds[:10],\n",
    "                'true_orders': true_orders,\n",
    "                'pred_orders':pred_orders\n",
    "            }\n",
    "            \n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20125f68-f7c4-4278-8d25-d3e8ace49f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_delta_date_str(date_str, delta):\n",
    "    return  (datetime.datetime.strptime(date_str, \"%Y-%m-%d\") + datetime.timedelta(days=delta)).strftime('%Y-%m-%d')\n",
    "\n",
    "def str2date(date_str):\n",
    "    return datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "def date2str(date):\n",
    "    return date.strftime('%Y-%m-%d')\n",
    "\n",
    "def get_train_test_data(data, startdate, enddate, testdate):\n",
    "    train_data = data[\n",
    "        (data['activate_date'] >= startdate) & (data['activate_date'] <= enddate)].reset_index(\n",
    "        drop=True).copy() \n",
    "\n",
    "    test_data = data[\n",
    "        (data['activate_date'] == testdate)].reset_index(drop=True).copy()  \n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def process_mask_col(train_data, mask_dates):\n",
    "    shape = len(train_data)\n",
    "    train_data['mask'] = [np.ones((7,))] * shape\n",
    "\n",
    "    train_data, recent_data = train_data[~train_data.dt.isin(mask_dates)] , train_data[train_data.dt.isin(mask_dates)]\n",
    "    train_data, valid_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "    \n",
    "    \n",
    "    for i, msk_dt in enumerate(mask_dates):\n",
    "        temp_mask = np.concatenate([np.ones((i + 1,)) , np.zeros((7 - i - 1,))])\n",
    "        recent_data.loc[recent_data.dt == msk_dt, 'mask'] = recent_data.loc[recent_data.dt == msk_dt].apply(lambda row: temp_mask, axis = 1)\n",
    "    train_data = pd.concat([train_data, recent_data], ignore_index=True)\n",
    "    print(\"数据集信息...\" )\n",
    "    # print(f\"训练集： {train_data.dt.value_counts()}, 验证集： {valid_data.dt.value_counts()}\")\n",
    "    return train_data , valid_data\n",
    "\n",
    "def create_tf_dataset(data, features, batch):\n",
    "    train_inputs = {name: tf.constant(v.values) if name != 'mask' else tf.constant(np.array(v.tolist())) \n",
    "                    for name, v in data[features + label_col].items()}\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_inputs)                                    \n",
    "    return train_dataset.batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f002ae4-e126-44c5-9a8f-08af5274ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练基本配置 \n",
    "res_csv = None\n",
    "folder_name = 'deep_res'\n",
    "Pred_Days = 10\n",
    "Pred_Date = '2023-05-01'\n",
    "label_col = ['label_1','label_2','label_3','label_4','label_5','label_6','label']\n",
    "# 处理缺失值\n",
    "import platform \n",
    "\n",
    "data = pd.read_csv(\"../data/liuliang_data/完整toy_liuliang_data.csv\", index_col= 0)\n",
    "data = data[data.dt > '2023-04-13']\n",
    "data.loc[:, features] = data.loc[:, features].fillna(0)\n",
    "\n",
    "train_days = 20\n",
    "\n",
    "dense_features = [feature for feature in features if feature not in category_features]\n",
    "sparse_features, dense_features, price_feature = category_features, dense_features, price_fatures\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b0046-271a-4fd9-a0db-1d4a5bb50b72",
   "metadata": {},
   "source": [
    "#### debug 把头只保留一最后一个 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bfa257e-b661-4ec9-9ce8-d60bd766f110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集开始：2023-04-04, 训练集结束：2023-04-30, 测试集：2023-05-01\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.0823, Val Loss = 0.1326\n",
      "Epoch 20: Loss = 0.0753, Val Loss = 0.1320\n",
      "Epoch 30: Loss = 0.0702, Val Loss = 0.1318\n",
      "Epoch 40: Loss = 0.0661, Val Loss = 0.1320\n",
      "Epoch 50: Loss = 0.0626, Val Loss = 0.1319\n",
      "bias: 0.25350651144981384\n",
      "AUC: 0.7362075512653528\n",
      "preds: [0.02053908072412014, 0.015673767775297165, 0.014595883898437023, 0.015665803104639053, 0.03495329990983009, 0.03912032023072243, 0.029007256031036377, 0.030131971463561058, 0.06108703836798668, 0.03760207071900368]\n",
      "true_orders: 641.0\n",
      "pred_orders: 803.4976806640625\n",
      "训练集开始：2023-04-05, 训练集结束：2023-05-01, 测试集：2023-05-02\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.1452, Val Loss = 0.1327\n",
      "Epoch 20: Loss = 0.1422, Val Loss = 0.1318\n",
      "Epoch 30: Loss = 0.1398, Val Loss = 0.1316\n",
      "Epoch 40: Loss = 0.1375, Val Loss = 0.1317\n",
      "Epoch 50: Loss = 0.1353, Val Loss = 0.1320\n",
      "bias: 0.2856878638267517\n",
      "AUC: 0.7344888737059306\n",
      "preds: [0.010641887784004211, 0.03650704026222229, 0.0281660296022892, 0.2942400276660919, 0.01094540860503912, 0.06660491973161697, 0.005312184803187847, 0.06169208884239197, 0.0257292240858078, 0.0343855544924736]\n",
      "true_orders: 631.0\n",
      "pred_orders: 811.26904296875\n",
      "训练集开始：2023-04-06, 训练集结束：2023-05-02, 测试集：2023-05-03\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.1209, Val Loss = 0.1365\n",
      "Epoch 20: Loss = 0.1202, Val Loss = 0.1368\n",
      "Epoch 30: Loss = 0.1194, Val Loss = 0.1371\n",
      "Epoch 40: Loss = 0.1190, Val Loss = 0.1378\n",
      "bias: 0.42146849632263184\n",
      "AUC: 0.7297704238989349\n",
      "preds: [0.012730916030704975, 0.01851612702012062, 0.04436993598937988, 0.2319244146347046, 0.205414816737175, 0.01433314848691225, 0.019369561225175858, 0.01361274067312479, 0.04901988059282303, 0.03553774207830429]\n",
      "true_orders: 618.0\n",
      "pred_orders: 878.467529296875\n",
      "训练集开始：2023-04-07, 训练集结束：2023-05-03, 测试集：2023-05-04\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.1375, Val Loss = 0.1360\n",
      "Epoch 20: Loss = 0.1361, Val Loss = 0.1355\n",
      "Epoch 30: Loss = 0.1358, Val Loss = 0.1355\n",
      "Epoch 40: Loss = 0.1353, Val Loss = 0.1358\n",
      "Epoch 50: Loss = 0.1343, Val Loss = 0.1365\n",
      "bias: 0.40943118929862976\n",
      "AUC: 0.7563327483653115\n",
      "preds: [0.013178100809454918, 0.01602364331483841, 0.09830138087272644, 0.005642165429890156, 0.09637244045734406, 0.041819825768470764, 0.07213348895311356, 0.0747310146689415, 0.026470128446817398, 0.040900010615587234]\n",
      "true_orders: 646.0\n",
      "pred_orders: 910.4925537109375\n",
      "训练集开始：2023-04-08, 训练集结束：2023-05-04, 测试集：2023-05-05\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.1427, Val Loss = 0.1304\n",
      "Epoch 20: Loss = 0.1389, Val Loss = 0.1300\n",
      "Epoch 30: Loss = 0.1364, Val Loss = 0.1301\n",
      "Epoch 40: Loss = 0.1345, Val Loss = 0.1304\n",
      "Epoch 50: Loss = 0.1317, Val Loss = 0.1309\n",
      "bias: 0.5171292424201965\n",
      "AUC: 0.7305061301034257\n",
      "preds: [0.08722472190856934, 0.028526458889245987, 0.023839909583330154, 0.20900900661945343, 0.0013840035535395145, 0.0826476514339447, 0.11817220598459244, 0.03515585884451866, 0.15070869028568268, 0.0017006121343001723]\n",
      "true_orders: 575.0\n",
      "pred_orders: 872.3493041992188\n",
      "训练集开始：2023-04-09, 训练集结束：2023-05-05, 测试集：2023-05-06\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.1517, Val Loss = 0.1375\n",
      "Epoch 20: Loss = 0.1471, Val Loss = 0.1369\n",
      "Epoch 30: Loss = 0.1426, Val Loss = 0.1368\n",
      "Epoch 40: Loss = 0.1381, Val Loss = 0.1368\n",
      "Epoch 50: Loss = 0.1332, Val Loss = 0.1373\n",
      "bias: 0.2864507734775543\n",
      "AUC: 0.7476186352615172\n",
      "preds: [0.021555829793214798, 0.017953602597117424, 0.00970548763871193, 0.04336190223693848, 0.006893296260386705, 0.03776226192712784, 0.019588740542531013, 0.03915039077401161, 0.014230971224606037, 0.20401889085769653]\n",
      "true_orders: 516.0\n",
      "pred_orders: 663.80859375\n",
      "训练集开始：2023-04-10, 训练集结束：2023-05-06, 测试集：2023-05-07\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.1871, Val Loss = 0.1340\n",
      "Epoch 20: Loss = 0.1569, Val Loss = 0.1338\n",
      "Epoch 30: Loss = 0.1246, Val Loss = 0.1340\n",
      "Epoch 40: Loss = 0.0939, Val Loss = 0.1341\n",
      "Epoch 50: Loss = 0.0734, Val Loss = 0.1348\n",
      "bias: 0.36138173937797546\n",
      "AUC: 0.7338159171789065\n",
      "preds: [0.017261257395148277, 0.2569345533847809, 0.11888206750154495, 0.011240066029131413, 0.03410814702510834, 0.02696380764245987, 0.055708445608615875, 0.03269295021891594, 0.025360707193613052, 0.09632684290409088]\n",
      "true_orders: 535.0\n",
      "pred_orders: 728.3392333984375\n",
      "训练集开始：2023-04-11, 训练集结束：2023-05-07, 测试集：2023-05-08\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.1220, Val Loss = 0.1357\n",
      "Epoch 20: Loss = 0.1201, Val Loss = 0.1354\n",
      "Epoch 30: Loss = 0.1190, Val Loss = 0.1357\n",
      "Epoch 40: Loss = 0.1178, Val Loss = 0.1360\n",
      "Epoch 50: Loss = 0.1163, Val Loss = 0.1362\n",
      "bias: 0.1446094810962677\n",
      "AUC: 0.7412444731097405\n",
      "preds: [0.03724856674671173, 0.0015725146513432264, 0.18768954277038574, 0.08384744077920914, 0.022742092609405518, 0.02213411219418049, 0.04246705397963524, 0.03083142451941967, 0.008861616253852844, 0.0331038236618042]\n",
      "true_orders: 511.0\n",
      "pred_orders: 584.8954467773438\n",
      "训练集开始：2023-04-12, 训练集结束：2023-05-08, 测试集：2023-05-09\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.1255, Val Loss = 0.1340\n",
      "Epoch 20: Loss = 0.1237, Val Loss = 0.1336\n",
      "Epoch 30: Loss = 0.1223, Val Loss = 0.1335\n",
      "Epoch 40: Loss = 0.1214, Val Loss = 0.1336\n",
      "Epoch 50: Loss = 0.1200, Val Loss = 0.1338\n",
      "bias: -0.01395275630056858\n",
      "AUC: 0.7272667596307996\n",
      "preds: [0.02943090721964836, 0.061325814574956894, 0.02454296685755253, 0.07121694833040237, 0.025881415233016014, 0.02666923590004444, 0.0408535897731781, 0.006754336878657341, 0.05363906919956207, 0.05113447830080986]\n",
      "true_orders: 510.0\n",
      "pred_orders: 502.88409423828125\n",
      "训练集开始：2023-04-13, 训练集结束：2023-05-09, 测试集：2023-05-10\n",
      "数据集信息...\n",
      "Epoch 10: Loss = 0.0310, Val Loss = 0.1348\n",
      "Epoch 20: Loss = 0.0256, Val Loss = 0.1338\n",
      "Epoch 30: Loss = 0.0221, Val Loss = 0.1333\n",
      "Epoch 40: Loss = 0.0196, Val Loss = 0.1331\n",
      "Epoch 50: Loss = 0.0175, Val Loss = 0.1331\n",
      "bias: -0.16326725482940674\n",
      "AUC: 0.7439860359746796\n",
      "preds: [0.04808155447244644, 0.2687601149082184, 0.020514260977506638, 0.019624721258878708, 0.012009155005216599, 0.015491650439798832, 0.06713491678237915, 0.24901729822158813, 0.07521185278892517, 0.09123935550451279]\n",
      "true_orders: 531.0\n",
      "pred_orders: 444.3050842285156\n"
     ]
    }
   ],
   "source": [
    "class EpochLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 10 == 0:  # 每 10 个 epoch 输出一次\n",
    "            print(f\"Epoch {epoch + 1}: Loss = {logs['loss']:.4f}, Val Loss = {logs.get('val_loss', 'N/A'):.4f}\")\n",
    "\n",
    "\n",
    "def train_model(data):\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',       \n",
    "        patience=30,               \n",
    "        restore_best_weights=True )\n",
    "\n",
    "    for delta in range(0, Pred_Days):\n",
    "        delta_sample_days = train_days\n",
    "        \n",
    "        today = get_delta_date_str(Pred_Date, delta)\n",
    "        startdate, enddate, testdate = get_delta_date_str(today, -train_days - 7), get_delta_date_str(today, -1), today\n",
    "        \n",
    "        \n",
    "        print(f\"训练集开始：{startdate}, 训练集结束：{enddate}, 测试集：{testdate}\")\n",
    "        \n",
    "        #  在 mask_date中间的 全部用mask 日期填充\n",
    "        mask_dates = [date2str(j) for j in pd.date_range(str2date(get_delta_date_str(today, -1)),periods=6,freq='-1D')]\n",
    "\n",
    "        # 按照时间区间 获取 训练 测试集 \n",
    "        train_data, test_data = get_train_test_data(data, startdate, enddate, testdate)\n",
    "        # 给 train data 添加mask 列\n",
    "        train_data, valid_data = process_mask_col(train_data, mask_dates)\n",
    "        train_data = train_data.sample(frac = 1.0)\n",
    "        train_dataset = create_tf_dataset(train_data, features + ['mask'], 2048)\n",
    "        valid_dataset = create_tf_dataset(valid_data, features + ['mask'], 2048)\n",
    "        test_data = create_tf_dataset(test_data, features, 200000)\n",
    "        \n",
    "        # MODEL \n",
    "        model = EveryDayModel(sparse_features, dense_features, price_feature, label_col,[256,128,64] )\n",
    "        model.compile(loss = MultiLoss(), optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002) )\n",
    "\n",
    "        # debug \n",
    "        # model.fit(valid_dataset)\n",
    "        # print(model.evaluate(valid_dataset))\n",
    "        # for epoch in range(40):\n",
    "        #     model.fit(train_dataset, validation_data = valid_dataset, epochs=1,callbacks=[early_stopping]) \n",
    "        #     for i in range(7):\n",
    "        #         format_dict(model.predict(test_data, i))\n",
    "        \n",
    "        model.fit(train_dataset, validation_data = valid_dataset, epochs=50 , verbose=0 , callbacks=[EpochLogger(), early_stopping]) \n",
    "        format_dict(model.predict(test_data, -1))\n",
    "\n",
    "\n",
    "\n",
    "model = train_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3607e61b-f8dc-460b-ba0e-8e862b0d9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def format_dict(data: dict) -> str:\n",
    "    formatted_output = []\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, tf.Tensor):\n",
    "            formatted_output.append(f\"{key}: {value.numpy().tolist()}\")\n",
    "        else:\n",
    "            formatted_output.append(f\"{key}: {value}\")\n",
    "    print( \"\\n\".join(formatted_output))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8e6a4-9b0c-4611-af90-a2c28086422a",
   "metadata": {},
   "source": [
    "### 调整 预处理   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f93e9240-1b17-40aa-b45e-cf3e5ec2fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk 加上正则化的模型  且 不加log的深度模型 \n",
    "class Dense_Process_Layer(layers.Layer):\n",
    "    def __init__(self, sparse_features, dense_features, price_features):\n",
    "        super().__init__()\n",
    "        self.sparse_features = sparse_features\n",
    "        self.dense_features = dense_features\n",
    "        self.price_features = price_features\n",
    "        self.concat_layer = layers.Concatenate()  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        concat_numeric = []\n",
    "        for name, input in inputs.items():\n",
    "            if name in self.dense_features:\n",
    "                input_cast = tf.cast(input, tf.float32)  \n",
    "                if name not in self.price_features:\n",
    "                    temp_feature = input_cast \n",
    "                else:\n",
    "                    temp_feature = input_cast\n",
    "                temp_feature = tf.expand_dims(temp_feature, 1)\n",
    "                concat_numeric.append(temp_feature)\n",
    "\n",
    "        return self.concat_layer(concat_numeric)  \n",
    "\n",
    "def process_mask_col(train_data, mask_dates):\n",
    "    shape = len(train_data)\n",
    "    train_data['mask'] = [np.ones((7,))] * shape\n",
    "\n",
    "    train_data, recent_data = train_data[~train_data.dt.isin(mask_dates)] , train_data[train_data.dt.isin(mask_dates)]\n",
    "    train_data, valid_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "    \n",
    "    recent_data = None\n",
    "    for i, msk_dt in enumerate(mask_dates):\n",
    "        temp_mask = np.concatenate([np.ones((i + 1,)) , np.zeros((7 - i - 1,))])\n",
    "    train_data = pd.concat([train_data, recent_data], ignore_index=True)\n",
    "    print(\"数据集信息...\" )\n",
    "    # print(f\"训练集： {train_data.dt.value_counts()}, 验证集： {valid_data.dt.value_counts()}\")\n",
    "    return train_data , valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e96ccfb9-7887-4fd2-802b-b017d5554e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集开始：2023-04-04, 训练集结束：2023-04-30, 测试集：2023-05-01\n",
      "数据集信息...\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 9s 114ms/step - loss: 0.2573 - val_loss: 0.1861\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1569 - val_loss: 0.1464\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1417 - val_loss: 0.1409\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1380 - val_loss: 0.1382\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1357 - val_loss: 0.1364\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1340 - val_loss: 0.1352\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1328 - val_loss: 0.1343\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1318 - val_loss: 0.1337\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.1310 - val_loss: 0.1332\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1303 - val_loss: 0.1328\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1297 - val_loss: 0.1324\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1291 - val_loss: 0.1321\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1286 - val_loss: 0.1318\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1282 - val_loss: 0.1315\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1277 - val_loss: 0.1313\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1273 - val_loss: 0.1312\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1270 - val_loss: 0.1310\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1266 - val_loss: 0.1308\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1263 - val_loss: 0.1307\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1259 - val_loss: 0.1306\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1256 - val_loss: 0.1305\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1253 - val_loss: 0.1304\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1251 - val_loss: 0.1304\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1248 - val_loss: 0.1303\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1245 - val_loss: 0.1303\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1243 - val_loss: 0.1302\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1240 - val_loss: 0.1302\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1238 - val_loss: 0.1302\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1235 - val_loss: 0.1302\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1233 - val_loss: 0.1301\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1231 - val_loss: 0.1301\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1229 - val_loss: 0.1301\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1227 - val_loss: 0.1301\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1224 - val_loss: 0.1301\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1222 - val_loss: 0.1301\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1220 - val_loss: 0.1301\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 7s 110ms/step - loss: 0.1218 - val_loss: 0.1301\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1216 - val_loss: 0.1301\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1214 - val_loss: 0.1301\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1212 - val_loss: 0.1301\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.1210 - val_loss: 0.1301\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 7s 121ms/step - loss: 0.1208 - val_loss: 0.1302\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1206 - val_loss: 0.1302\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1204 - val_loss: 0.1302\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1202 - val_loss: 0.1302\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1200 - val_loss: 0.1303\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1198 - val_loss: 0.1303\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1196 - val_loss: 0.1304\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1194 - val_loss: 0.1304\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1192 - val_loss: 0.1305\n",
      "bias: 0.02885429933667183\n",
      "AUC: 0.75652872389508\n",
      "preds: [0.012203136458992958, 0.011783233843743801, 0.011699656024575233, 0.013902286998927593, 0.017501840367913246, 0.021979747340083122, 0.014634850434958935, 0.022715294733643532, 0.06065065786242485, 0.023245107382535934]\n",
      "true_orders: 641.0\n",
      "pred_orders: 659.49560546875\n"
     ]
    }
   ],
   "source": [
    "# 初始化 StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',       \n",
    "    patience=10,               \n",
    "    restore_best_weights=True )\n",
    "\n",
    "delta = 0\n",
    "delta_sample_days = train_days\n",
    "\n",
    "today = get_delta_date_str(Pred_Date, delta)\n",
    "startdate, enddate, testdate = get_delta_date_str(today, -train_days - 7), get_delta_date_str(today, -1), today\n",
    "\n",
    "\n",
    "print(f\"训练集开始：{startdate}, 训练集结束：{enddate}, 测试集：{testdate}\")\n",
    "\n",
    "#  在 mask_date中间的 全部用mask 日期填充\n",
    "mask_dates = [date2str(j) for j in pd.date_range(str2date(get_delta_date_str(today, -1)),periods=6,freq='-1D')]\n",
    "\n",
    "# 按照时间区间 获取 训练 测试集 \n",
    "train_data, test_data = get_train_test_data(data, startdate, enddate, testdate)\n",
    "\n",
    "\n",
    "# 对测试集的指定列进行 Z-score 标准化（使用训练集的均值和标准差）\n",
    "train_data[dense_features] = scaler.fit_transform(train_data[dense_features])\n",
    "test_data[dense_features] = scaler.transform(test_data[dense_features])\n",
    "\n",
    "# 给 train data 添加mask 列\n",
    "train_data, valid_data = process_mask_col(train_data, mask_dates)\n",
    "\n",
    "train_dataset = create_tf_dataset(train_data, features + ['mask'], 2048)\n",
    "valid_dataset = create_tf_dataset(valid_data, features + ['mask'], 2048)\n",
    "test_data = create_tf_dataset(test_data, features, 200000)\n",
    "\n",
    "# MODEL \n",
    "model = EveryDayModel(sparse_features, dense_features, price_feature, label_col,[256,128,64] )\n",
    "model.compile(loss = MultiLoss(), optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001) )\n",
    "\n",
    "# debug \n",
    "# model.fit(valid_dataset)\n",
    "# print(model.evaluate(valid_dataset))\n",
    "model.fit(train_dataset, validation_data = valid_dataset, epochs=50\n",
    "          # ,callbacks=[early_stopping]\n",
    "         ) \n",
    "format_dict(model.predict(test_data, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48201c27-42ef-41ae-98ba-3b9e51ea29e7",
   "metadata": {},
   "source": [
    "## 结论： 近期样本的加入，AUC能提升近1个点，但是 bias 也会扩大。主要是近期样本的 下单率确实和 预测日gap较大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3a6fbc4-af9d-4503-92ec-317f81c97325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1222 - val_loss: 0.1303\n",
      "Epoch 2/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1220 - val_loss: 0.1303\n",
      "Epoch 3/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1218 - val_loss: 0.1303\n",
      "Epoch 4/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1217 - val_loss: 0.1303\n",
      "Epoch 5/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1215 - val_loss: 0.1304\n",
      "Epoch 6/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1213 - val_loss: 0.1304\n",
      "Epoch 7/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1211 - val_loss: 0.1304\n",
      "Epoch 8/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1209 - val_loss: 0.1305\n",
      "Epoch 9/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1207 - val_loss: 0.1305\n",
      "Epoch 10/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1205 - val_loss: 0.1305\n",
      "Epoch 11/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1203 - val_loss: 0.1306\n",
      "Epoch 12/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1201 - val_loss: 0.1306\n",
      "Epoch 13/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1199 - val_loss: 0.1307\n",
      "Epoch 14/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1197 - val_loss: 0.1307\n",
      "Epoch 15/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1195 - val_loss: 0.1308\n",
      "Epoch 16/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1193 - val_loss: 0.1308\n",
      "Epoch 17/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1191 - val_loss: 0.1309\n",
      "Epoch 18/40\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1189 - val_loss: 0.1309\n",
      "Epoch 19/40\n",
      "61/61 [==============================] - 7s 117ms/step - loss: 0.1187 - val_loss: 0.1310\n",
      "Epoch 20/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1185 - val_loss: 0.1310\n",
      "Epoch 21/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1183 - val_loss: 0.1311\n",
      "Epoch 22/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1181 - val_loss: 0.1312\n",
      "Epoch 23/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1179 - val_loss: 0.1313\n",
      "Epoch 24/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1177 - val_loss: 0.1313\n",
      "Epoch 25/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1175 - val_loss: 0.1314\n",
      "Epoch 26/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1173 - val_loss: 0.1315\n",
      "Epoch 27/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1171 - val_loss: 0.1316\n",
      "Epoch 28/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1169 - val_loss: 0.1316\n",
      "Epoch 29/40\n",
      "61/61 [==============================] - 7s 110ms/step - loss: 0.1167 - val_loss: 0.1317\n",
      "Epoch 30/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1165 - val_loss: 0.1318\n",
      "Epoch 31/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1163 - val_loss: 0.1319\n",
      "Epoch 32/40\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1160 - val_loss: 0.1320\n",
      "Epoch 33/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1158 - val_loss: 0.1321\n",
      "Epoch 34/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1156 - val_loss: 0.1322\n",
      "Epoch 35/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1154 - val_loss: 0.1322\n",
      "Epoch 36/40\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.1152 - val_loss: 0.1323\n",
      "Epoch 37/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1150 - val_loss: 0.1325\n",
      "Epoch 38/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1147 - val_loss: 0.1325\n",
      "Epoch 39/40\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1145 - val_loss: 0.1327\n",
      "Epoch 40/40\n",
      "61/61 [==============================] - 7s 110ms/step - loss: 0.1143 - val_loss: 0.1328\n",
      "bias: 0.012142663821578026\n",
      "AUC: 0.7474684321316727\n",
      "preds: [0.01302121952176094, 0.012627107091248035, 0.00888181384652853, 0.01409940142184496, 0.017727429047226906, 0.04538991302251816, 0.01447009202092886, 0.020062247291207314, 0.0710325837135315, 0.021725211292505264]\n",
      "true_orders: 641.0\n",
      "pred_orders: 648.783447265625\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data = valid_dataset, epochs=40) \n",
    "format_dict(model.predict(test_data, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e75a1cd-b66b-4609-8ad9-3adb2a8a11ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1132 - val_loss: 0.1334\n",
      "bias: 0.016261251643300056\n",
      "AUC: 0.7453388198980566\n",
      "preds: [0.01309780403971672, 0.01244854461401701, 0.008600215427577496, 0.013786629773676395, 0.017676163464784622, 0.047614872455596924, 0.014451979659497738, 0.019909843802452087, 0.07463574409484863, 0.021418534219264984]\n",
      "true_orders: 641.0\n",
      "pred_orders: 651.4234619140625\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data = valid_dataset, epochs=1) \n",
    "format_dict(model.predict(test_data, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ff2cf-2495-4c86-9091-7ec8b41d2777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d657b7f-46ec-448d-9d42-ecbf8594b400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
